<div align="left">Связь условной вероятности с совместной</div>	<div align="left">\(p(y_j|x_i)={p(x_i,y_j)\over p(x_i)}\)</div>

<div align="left">Связь совместной вероятности с условной</div>	<div align="left">\(p(x_i,y_j)=p(x_i|y_j)\cdot p(y_j)\)</div>

<div align="left">Связь условной и апостериорной вероятностей</div>	<div align="left">\(p(x_i|y_j)p(y_j)=p(y_j|x_i)p(x_i)\)</div>

<div align="left">Априорная вероятность появления символа на выходе</div>	<div align="left">\(p(y_j)=\sum_{i=0}^{n-1}{p(x_i,y_j)}\)</div>

<div align="left">Условие нормировки условной вероятности</div>	<div align="left">\(\sum_{j=0}^{m-1}{p(y_j|x_i)}=1\)</div>

<div align="left">Взаимная информация I(x,y)</div>	<div align="left">количество информации, доставляемое событием y о событии x (эквивалентно количеству информации, доставляемому событием x о событии y) <br> \(I(x_i;y_j)=\log_2\left({p(x_i|y_i))\over p(x_i)}\right)\)</div>

<div align="left">Собственная информация события</div>	<div align="left">количество информации, доставляемое самим событием x <br> \(I(x_i)=-\log_2 p(x_i)\)</div>

<div align="left">Собственная информация совместного события x и y</div>	<div align="left">количество информации, доставляемое совокупностью событий x и y, отличается от взаимной информации I(x;y) <br> \(I(x_i\, y_j) = -\log_2(p(x_i,y_j)\)</div>

<div align="left">Условная собственная информация события x</div>	<div align="left">характеризует максимальное количество информации, которое способно доставить само событие x при условии, что событие y известно <br> \(I(x_i|y_j)=-\log_2(p(x_i|y_j)\)</div>

<div align="left">Среднее количество взаимной информации</div>	<div align="left">\(I(X;Y) = \sum_{i=0}^{n-1}{\sum_{j=0}^{m-1}{p(x_i,y_j)I(x_i;y_j)}} = \sum_{i=0}^{n-1}{\sum_{j=0}^{m-1}{p(x_i,y_j)(-\log_2(p(x_i,y_j))}}\)</div>

<div align="left">Энтропия</div>	<div align="left">средняя собственная информация источника <br> \(H(x) = I(X) = - \sum_{i=0}^{n-1}{p(x_i)\log(p(x_i))}\)</div>

<div align="left">Избыточность источника</div>	<div align="left">\(\rho=1-{H(X)\over H_{max}(X)}=1 - {H(X)\over \log_2(n)}\)</div>

<div align="left">Условная энтропия</div>	<div align="left">количество информации, которая характеризует степень неопределенности значений случайной величины Y, остающуюся после того, как стало известно значение, принятое случайной величиной X <br> \(H(X|Y)=\sum_{i=0}^{n-1}{\sum_{j=0}^{m-1}{p(x_i,y_j)\log_2(p(x_i|y_j))}}\)</div>

